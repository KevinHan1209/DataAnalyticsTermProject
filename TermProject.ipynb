{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Project\n",
    "\n",
    "## Kevin Han, kh38823\n",
    "\n",
    "## Using the adult dataset: https://archive.ics.uci.edu/dataset/2/adult. Predicts whether income exceeds $50k per year based on variety of factors, such as education, age, workclass, race, sex, etc. \n",
    "\n",
    "My learning model is binary classification. I will be implementing a neural network. My current expectation of the results is that it would provide a pretty good prediction of the target variable that is whether or not any given adult exceeds $50k per year in income. However, since a neural network is a black box with many layers, it is practically impossible to evaluate which parameters/features are the best predictors of the target variables. This is only of the largest disadvantages to neural networks in my opinion. It is a powerful predictor but a very poor interpreter of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
      "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
      "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "  \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = pd.DataFrame(adult.data.features)\n",
    "y = pd.DataFrame(adult.data.targets)\n",
    "\n",
    "# Clean up dataset and hot encode. Not sure why some values have a period and others do not, but I'm going to treat them as the same.\n",
    "# Hot encode categorical target variable\n",
    "y['income'] = y['income'].map({'<=50K': 0, '<=50K.': 0, '>50K': 1, '>50K.': 1})\n",
    "print(X.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country']\n",
      "Mapping for 'workclass': {'?': 0, 'Federal-gov': 1, 'Local-gov': 2, 'Never-worked': 3, 'Private': 4, 'Self-emp-inc': 5, 'Self-emp-not-inc': 6, 'State-gov': 7, 'Without-pay': 8, nan: 9}\n",
      "Mapping for 'education': {'10th': 0, '11th': 1, '12th': 2, '1st-4th': 3, '5th-6th': 4, '7th-8th': 5, '9th': 6, 'Assoc-acdm': 7, 'Assoc-voc': 8, 'Bachelors': 9, 'Doctorate': 10, 'HS-grad': 11, 'Masters': 12, 'Preschool': 13, 'Prof-school': 14, 'Some-college': 15}\n",
      "Mapping for 'marital-status': {'Divorced': 0, 'Married-AF-spouse': 1, 'Married-civ-spouse': 2, 'Married-spouse-absent': 3, 'Never-married': 4, 'Separated': 5, 'Widowed': 6}\n",
      "Mapping for 'occupation': {'?': 0, 'Adm-clerical': 1, 'Armed-Forces': 2, 'Craft-repair': 3, 'Exec-managerial': 4, 'Farming-fishing': 5, 'Handlers-cleaners': 6, 'Machine-op-inspct': 7, 'Other-service': 8, 'Priv-house-serv': 9, 'Prof-specialty': 10, 'Protective-serv': 11, 'Sales': 12, 'Tech-support': 13, 'Transport-moving': 14, nan: 15}\n",
      "Mapping for 'relationship': {'Husband': 0, 'Not-in-family': 1, 'Other-relative': 2, 'Own-child': 3, 'Unmarried': 4, 'Wife': 5}\n",
      "Mapping for 'race': {'Amer-Indian-Eskimo': 0, 'Asian-Pac-Islander': 1, 'Black': 2, 'Other': 3, 'White': 4}\n",
      "Mapping for 'sex': {'Female': 0, 'Male': 1}\n",
      "Mapping for 'native-country': {'?': 0, 'Cambodia': 1, 'Canada': 2, 'China': 3, 'Columbia': 4, 'Cuba': 5, 'Dominican-Republic': 6, 'Ecuador': 7, 'El-Salvador': 8, 'England': 9, 'France': 10, 'Germany': 11, 'Greece': 12, 'Guatemala': 13, 'Haiti': 14, 'Holand-Netherlands': 15, 'Honduras': 16, 'Hong': 17, 'Hungary': 18, 'India': 19, 'Iran': 20, 'Ireland': 21, 'Italy': 22, 'Jamaica': 23, 'Japan': 24, 'Laos': 25, 'Mexico': 26, 'Nicaragua': 27, 'Outlying-US(Guam-USVI-etc)': 28, 'Peru': 29, 'Philippines': 30, 'Poland': 31, 'Portugal': 32, 'Puerto-Rico': 33, 'Scotland': 34, 'South': 35, 'Taiwan': 36, 'Thailand': 37, 'Trinadad&Tobago': 38, 'United-States': 39, 'Vietnam': 40, 'Yugoslavia': 41, nan: 42}\n"
     ]
    }
   ],
   "source": [
    "# Look at categorical variables in X. Use LabelEncoder. \n",
    "\n",
    "# Look at which variables would be categorical in X\n",
    "print(X.columns.tolist())\n",
    "\n",
    "# Define the categorical variables\n",
    "categorical_var = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Define a dictionary to store the mapping of the encoding for future reference\n",
    "encoding_mapping = {}\n",
    "\n",
    "# Loop through each categorical variable\n",
    "for var in categorical_var:\n",
    "    # Fit and transform the variable, and store the encoded values in X_encoded\n",
    "    X[var] = label_encoder.fit_transform(X[var])\n",
    "    \n",
    "    # Store the mapping information in the dictionary\n",
    "    encoding_mapping[var] = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "# Print the mapping information for each variable\n",
    "for var, mapping in encoding_mapping.items():\n",
    "    print(f\"Mapping for '{var}': {mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate X and y for easy stratified sampling\n",
    "data_df = pd.DataFrame(X, columns=adult.data.feature_names)\n",
    "data_df['income'] = y\n",
    "\n",
    "# Drop question marks\n",
    "data_df = data_df[(data_df['workclass'] != 0) & (data_df['occupation'] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    35503\n",
      "1    11496\n",
      "Name: income, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data_df['income'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prevent skewing the dataset, I want equal samples of both <=50K and >50K incomes in both my training and testing datasets\n",
    "\n",
    "# Split the dataset based on 'income' column\n",
    "income_0_df = data_df[data_df['income'] == 0]\n",
    "income_1_df = data_df[data_df['income'] == 1]\n",
    "\n",
    "# Randomly sample 1600 instances from each income group for training data\n",
    "sub50k_training = income_0_df.sample(n=1600, random_state=123)\n",
    "over50k_training = income_1_df.sample(n=1600, random_state=123)\n",
    "\n",
    "# Subtract the already sampled instances from main dataframe\n",
    "remaining_data_df = data_df.drop(index=sub50k_training.index)\n",
    "remaining_data_df = remaining_data_df.drop(index=over50k_training.index)\n",
    "remaining_income_0 = remaining_data_df[remaining_data_df['income'] == 0]\n",
    "remaining_income_1 = remaining_data_df[remaining_data_df['income'] == 1]\n",
    "\n",
    "# Randomly sample 400 instances for testing data\n",
    "sub50k_testing = remaining_income_0.sample(n=400, random_state=123)\n",
    "over50k_testing = remaining_income_1.sample(n=400, random_state=123)\n",
    "\n",
    "# Define the training and testing datasets\n",
    "combined_training_df = pd.concat([sub50k_training, over50k_training], ignore_index=True)\n",
    "X_train = combined_training_df.drop('income', axis=1)\n",
    "y_train = combined_training_df['income']\n",
    "\n",
    "combined_testing_df = pd.concat([sub50k_testing, over50k_testing], ignore_index=True)\n",
    "X_test = combined_testing_df.drop('income', axis=1)\n",
    "y_test = combined_testing_df['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1600\n",
      "1    1600\n",
      "Name: income, dtype: int64\n",
      "0    400\n",
      "1    400\n",
      "Name: income, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have sampled well and defined the training and testing data, let's preprocess our data for the neural network\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Normalize the input features\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train.numpy())\n",
    "X_train = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
    "X_test_normalized = scaler.fit_transform(X_test.numpy())\n",
    "X_test = torch.tensor(X_test_normalized, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # Input layer to hidden layer\n",
    "        self.fc2 = nn.Linear(64, 32)          # Hidden layer to hidden layer\n",
    "        self.fc3 = nn.Linear(32, 1)           # Hidden layer to output layer\n",
    "        self.relu = nn.LeakyReLU(0.01)            # LeakyReLU activation function. Introduce non-linearity in order to learn a more complex relationship. Used Leaky to acount for negatives.\n",
    "        self.sigmoid = nn.Sigmoid()           # Sigmoid activation function, required for binary classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (fc1): Linear(in_features=14, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "model = NeuralNetwork(input_size)\n",
    "\n",
    "# Hyper-parameters\n",
    "weight_decay = 0.001 # For L2 regularization\n",
    "num_epochs = 100 # Number of iterations\n",
    "lr = 0.01 # Learning rate\n",
    "batch_size = 32 # batch size for mini-batch gradient descent \n",
    "alpha = 1.1  # Bold driver parameter for increasing LR\n",
    "beta = 0.5   # Bold driver parameter for decreasing LR\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)  # Adam optimizer\n",
    "\n",
    "# Initialize the previous loss for comparison\n",
    "prev_loss = float('inf')\n",
    "\n",
    "# Initialize the initial learning rate\n",
    "initial_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "# Create DataLoader for batched data\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize loss list for visualization\n",
    "losses = []\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.4041\n",
      "Epoch [20/100], Loss: 0.4039\n",
      "Epoch [30/100], Loss: 0.4039\n",
      "Epoch [40/100], Loss: 0.4039\n",
      "Epoch [50/100], Loss: 0.4039\n",
      "Epoch [60/100], Loss: 0.4039\n",
      "Epoch [70/100], Loss: 0.4039\n",
      "Epoch [80/100], Loss: 0.4039\n",
      "Epoch [90/100], Loss: 0.4039\n",
      "Epoch [100/100], Loss: 0.4039\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for inputs, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Add L2 regularization to the loss\n",
    "        l2_reg = 0\n",
    "        for param in model.parameters():\n",
    "            l2_reg += torch.norm(param)**2\n",
    "        loss += weight_decay * l2_reg \n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    # Calculate average loss per epoch\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    losses.append(epoch_loss)\n",
    "    # Print training loss every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Update learning rate based on loss change\n",
    "    if epoch_loss > prev_loss:\n",
    "        # Increase learning rate\n",
    "        optimizer.param_groups[0]['lr'] *= alpha\n",
    "    else:\n",
    "        # Decrease learning rate\n",
    "        optimizer.param_groups[0]['lr'] *= beta\n",
    "\n",
    "     # Update previous loss for the next epoch\n",
    "    prev_loss = epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHFCAYAAAAaD0bAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQNklEQVR4nO3deVxU5f4H8M9szACCisiiApIbKqkFabgrgmmZWd5MFLX0l4obkinmbi5ppdxuF+7VTG9XS3Kp601UMJdMLTdcSjPLBRe47oIiMDDn94fNsXHYmTkPyOf9evGKOXPOmed80fj4nOc8j0qSJAlERERE1YhadAOIiIiIlMYARERERNUOAxARERFVOwxAREREVO0wABEREVG1wwBERERE1Q4DEBEREVU7DEBERERU7TAAERERUbXDAEQkkEqlKtXXrl27KvQ5s2fPhkqlKtexu3btskkbKvLZ69evV/yzKxPzz6+or/PnzwttH39OVBVpRTeAqDrbv3+/xet3330XO3fuxI4dOyy2t2jRokKfM2LECDz33HPlOvbpp5/G/v37K9wGqritW7eiZs2aVtu9vb0FtIaoamMAIhLo2WeftXhdt25dqNVqq+2Pys7OhpOTU6k/p0GDBmjQoEG52ujq6lpie0gZQUFBcHd3F90MoscCb4ERVXJdu3ZFYGAgvvvuO7Rv3x5OTk544403AACJiYkIDw+Ht7c3HB0d0bx5c8TGxuLevXsW5yjsFljDhg3xwgsvYOvWrXj66afh6OiIgIAAfPrppxb7FXYLbNiwYahRowZ+++039O7dGzVq1ICPjw/eeust5ObmWhx/6dIl9O/fHy4uLqhVqxYGDRqEgwcPQqVSYdWqVTap0U8//YS+ffuidu3aMBgMaNOmDf71r39Z7GMymTBv3jw0a9YMjo6OqFWrFlq1aoW//vWv8j7Xrl3Dm2++CR8fH+j1etStWxcdOnTA9u3bi/zsr7/+GiqVCt9++63VewkJCVCpVDh+/DgA4OzZs3jttddQr1496PV6eHp6IjQ0FEePHrVJHc6fPw+VSoXFixdj/vz58PX1hcFgQHBwcKHt+/777xEaGgoXFxc4OTmhffv22Lx5s9V+ly9fluvi4OCAevXqoX///vjf//5nsZ/RaMS0adNQr149uLq6okePHjh9+rRNro3I1tgDRFQFpKenY/DgwZg8eTIWLFgAtfrBv13OnDmD3r17Izo6Gs7Ozvjll1+waNEiHDhwwOo2WmGOHTuGt956C7GxsfD09MQnn3yC4cOHo3HjxujcuXOxxxqNRrz44osYPnw43nrrLXz33Xd49913UbNmTcycORMAcO/ePXTr1g03b97EokWL0LhxY2zduhUDBgyoeFH+cPr0abRv3x4eHh746KOPUKdOHaxevRrDhg3D//73P0yePBkAsHjxYsyePRvTp09H586dYTQa8csvv+D27dvyuSIjI3HkyBHMnz8fTZs2xe3bt3HkyBHcuHGjyM9/4YUX4OHhgZUrVyI0NNTivVWrVuHpp59Gq1atAAC9e/dGQUEBFi9eDF9fX1y/fh379u2zaENxCgoKkJ+fb7FNpVJBo9FYbPv444/h5+eHuLg4mEwmLF68GL169cLu3bsREhICANi9ezfCwsLQqlUrrFixAnq9HvHx8ejTpw+++OIL+Wd0+fJlPPPMMzAajXjnnXfQqlUr3LhxA9u2bcOtW7fg6ekpf+4777yDDh064JNPPkFmZiamTJmCPn364NSpU1ZtJBJOIqJKY+jQoZKzs7PFti5dukgApG+//bbYY00mk2Q0GqXdu3dLAKRjx47J782aNUt69K+7n5+fZDAYpAsXLsjb7t+/L7m5uUkjR46Ut+3cuVMCIO3cudOinQCkL7/80uKcvXv3lpo1aya//vvf/y4BkLZs2WKx38iRIyUA0sqVK4u9JvNnr1u3rsh9XnvtNUmv10tpaWkW23v16iU5OTlJt2/fliRJkl544QWpTZs2xX5ejRo1pOjo6GL3KUxMTIzk6Ogof5YkSdLJkyclANLf/vY3SZIk6fr16xIAKS4ursznN//8Cvtq1KiRvN+5c+ckAFK9evWk+/fvy9szMzMlNzc3qUePHvK2Z599VvLw8JCysrLkbfn5+VJgYKDUoEEDyWQySZIkSW+88Yak0+mkkydPFtk+88+pd+/eFtu//PJLCYC0f//+Ml8zkb3xFhhRFVC7dm10797davvZs2cREREBLy8vaDQa6HQ6dOnSBQBw6tSpEs/bpk0b+Pr6yq8NBgOaNm2KCxculHisSqVCnz59LLa1atXK4tjdu3fDxcXFagD2wIEDSzx/ae3YsQOhoaHw8fGx2D5s2DBkZ2fLA83btm2LY8eOISoqCtu2bUNmZqbVudq2bYtVq1Zh3rx5+OGHH2A0GkvVhjfeeAP3799HYmKivG3lypXQ6/WIiIgAALi5uaFRo0Z4//33sWTJEqSmpsJkMpXpWrdv346DBw9afH399ddW+7388sswGAzyaxcXF/Tp0wffffcdCgoKcO/ePfz444/o378/atSoIe+n0WgQGRmJS5cuybeutmzZgm7duqF58+Yltu/FF1+0eG3u+SrNnycipTEAEVUBhT3lc/fuXXTq1Ak//vgj5s2bh127duHgwYPYuHEjAOD+/fslnrdOnTpW2/R6famOdXJysvglaz42JydHfn3jxg2LWyRmhW0rrxs3bhRan3r16snvA8DUqVPxwQcf4IcffkCvXr1Qp04dhIaG4tChQ/IxiYmJGDp0KD755BOEhITAzc0NQ4YMQUZGRrFtaNmyJZ555hmsXLkSwINbVatXr0bfvn3h5uYGAPI4oZ49e2Lx4sV4+umnUbduXYwfPx5ZWVmlutbWrVsjODjY4iswMNBqPy8vr0K35eXl4e7du7h16xYkSSpV3a5du1bqAfSP/nnS6/UASvdnkUhpDEBEVUBhc/js2LEDV65cwaeffooRI0agc+fOCA4OhouLi4AWFq5OnTpWA2UBlBgoyvoZ6enpVtuvXLkCAPJTU1qtFjExMThy5Ahu3ryJL774AhcvXkTPnj2RnZ0t7xsXF4fz58/jwoULWLhwITZu3Ihhw4aV2I7XX38dP/zwA06dOoWtW7ciPT0dr7/+usU+fn5+WLFiBTIyMnD69GlMnDgR8fHxePvttytYBUuF1TcjIwMODg6oUaMGateuDbVaXaq61a1bF5cuXbJp+4gqAwYgoirKHIrM/8o2++c//ymiOYXq0qULsrKysGXLFovta9eutdlnhIaGymHwzz777DM4OTkV+gh/rVq10L9/f4wZMwY3b94sdCJBX19fjB07FmFhYThy5EiJ7Rg4cCAMBgNWrVqFVatWoX79+ggPDy9y/6ZNm2L69Ol48sknS3X+sti4caNFT1xWVhb++9//olOnTtBoNHB2dka7du2wceNGi94Zk8mE1atXo0GDBmjatCkAoFevXti5cyef5qLHDp8CI6qi2rdvj9q1a2PUqFGYNWsWdDod1qxZg2PHjolummzo0KFYunQpBg8ejHnz5qFx48bYsmULtm3bBgDy02wl+eGHHwrd3qVLF8yaNQvffPMNunXrhpkzZ8LNzQ1r1qzB5s2bsXjxYnniwD59+iAwMBDBwcGoW7cuLly4gLi4OPj5+aFJkya4c+cOunXrhoiICAQEBMDFxQUHDx7E1q1b8fLLL5fYxlq1aqFfv35YtWoVbt++jUmTJllc3/HjxzF27Fj85S9/QZMmTeDg4IAdO3bg+PHjiI2NLVUdDh8+XOhEiC1atICrq6v8WqPRICwsDDExMTCZTFi0aBEyMzMxZ84ceZ+FCxciLCwM3bp1w6RJk+Dg4ID4+Hj89NNP+OKLL+SAPXfuXGzZsgWdO3fGO++8gyeffBK3b9/G1q1bERMTg4CAgFK1naiyYQAiqqLq1KmDzZs346233sLgwYPh7OyMvn37IjExEU8//bTo5gEAnJ2dsWPHDkRHR2Py5MlQqVQIDw9HfHw8evfujVq1apXqPB9++GGh23fu3ImuXbti3759eOeddzBmzBjcv38fzZs3x8qVKy1uXXXr1g0bNmyQH9H28vJCWFgYZsyYAZ1OB4PBgHbt2uHf//43zp8/D6PRCF9fX0yZMkV+lL4kr7/+Or744gsAsLpt5uXlhUaNGiE+Ph4XL16ESqXCE088gQ8//BDjxo0r1fmLms07JSUFPXr0kF+PHTsWOTk5GD9+PK5evYqWLVti8+bN6NChg7xPly5dsGPHDsyaNQvDhg2DyWRC69atsWnTJrzwwgvyfvXr18eBAwcwa9YsvPfee7hx4wbq1q2Ljh07yuObiKoilSRJkuhGEFH1smDBAkyfPh1paWnlnqGarJ0/fx7+/v54//33MWnSJNHNIarU2ANERHb18ccfAwACAgJgNBqxY8cOfPTRRxg8eDDDDxEJwwBERHbl5OSEpUuX4vz588jNzZVvK02fPl1004ioGuMtMCIiIqp2+Bg8ERERVTsMQERERFTtMAARERFRtcNB0IUwmUy4cuUKXFxcCl2CgIiIiCofSZKQlZWFevXqlTjRKgNQIa5cuWK1sjQRERFVDRcvXixxmg0GoEKYF5O8ePGixfTyZWU0GpGcnIzw8HDodDpbNY8KwVorh7VWFuutHNZaOfaqdWZmJnx8fEq1KDQDUCHMt71cXV0rHICcnJzg6urKv0x2xlorh7VWFuutHNZaOfaudWmGr3AQNBEREVU7DEBERERU7TAAERERUbXDMUBERJVAQUEBjEaj1Xaj0QitVoucnBwUFBQIaFn1wVorpyK1dnBwKPER99JgACIiEkiSJGRkZOD27dtFvu/l5YWLFy9yXjI7Y62VU5Faq9Vq+Pv7w8HBoUJtYAAiIhLIHH48PDzg5ORk9cvAZDLh7t27qFGjhk3+1UtFY62VU95amycqTk9Ph6+vb4WCKgMQEZEgBQUFcvipU6dOofuYTCbk5eXBYDDwl7KdsdbKqUit69atiytXriA/P79Cj9DzJ0xEJIh5zI+Tk5PglhBVHeZbXxUdp8UAREQkGMebEJWerf6+MAARERFRtcMAREREwnXt2hXR0dGl3v/8+fNQqVQ4evSo3dpE9ify58gAREREpaZSqYr9GjZsWLnOu3HjRrz77rul3t/Hxwfp6ekIDAws1+eV1uMctLp27Vroz3DUqFGim6YIPgWmoNz8Aly/mwcVgHq1HEU3h4iozNLT0+XvExMTMXPmTJw+fVre5uho+f82o9FYqid13NzcytQOjUYDLy+vMh1D1v7v//4Pc+fOtdhWXQblswdIQScu3UGH93YgYvkPoptCRFQuXl5e8lfNmjWhUqnk1zk5OahVqxa+/PJLdO3aFQaDAatXr8aNGzcwcOBANGjQAE5OTnjyySfxxRdfWJz30VtgDRs2xIIFC/DGG2/AxcUFvr6+WLZsmfz+oz0zu3btgkqlwrfffovg4GA4OTmhffv2FuEMAObNmwcPDw+4uLhgxIgRiI2NRZs2bcpdj9zcXIwfPx4eHh4wGAzo2LEjDh48KL9/69YtDBo0CHXr1oWjoyOaNGmClStXAgDy8vIwduxYeHt7w2AwoGHDhli4cGGhn7Nt2zYYDAarCTPHjx+PLl26AAAuXLiAPn36oHbt2nB2dkbLli2RlJRUbPudnJwsfqZeXl5wdXUF8LDGa9euRfv27WEwGNCyZUvs2rXL4hy7d+9G27Ztodfr4e3tjdjYWOTn58vvm0wmLFq0CI0bN4Zer4evry8WLFhgcY6zZ8+iW7ducHJyQuvWrbF///5i220LDEAK0moelNtYIAluCRFVVpIkITsv3+Lrfl6B1TZbf0mS7f6/NGXKFIwfPx6nTp1Cz549kZOTg6CgIHzzzTf46aef8OabbyIyMhI//vhjsef58MMPERwcjNTUVERFRWH06NH45Zdfij1m2rRp+PDDD3Ho0CFotVq88cYb8ntr1qzB/PnzsWjRIhw+fBi+vr5ISEio0LVOnjwZGzZswL/+9S8cOXIEjRs3Rs+ePXHz5k0AwIwZM3Dy5Els2bIFp06dQkJCAtzd3QEAH330ETZt2oQvv/wSp0+fxurVq9GwYcNCP6dHjx6oVasWNmzYIG8rKCjAl19+iUGDBgEAxowZg9zcXHz33Xc4ceIEFi1ahBo1alTo+gDg7bffxltvvYXU1FS0b98eL774Im7cuAEAuHz5Mnr37o1nnnkGx44dQ0JCAlasWIF58+bJx0+dOhWLFi2Sa/H555/Dw8PD4jOmTZuGSZMm4ejRo2jatCkGDhxoEaLsgbfAFKTTPHh0z1hgEtwSIqqs7hsL0GLmNsU/9+TcnnBysM2vhOjoaLz88ssW2yZNmiR/P27cOGzduhXr1q1Du3btijxP7969ERUVBeBBqFq6dCl27dqFgICAIo+ZP3++3CMSGxuL559/Hjk5OTAYDPjb3/6G4cOH4/XXXwcAzJw5E8nJybh79265rvPevXtISEjAqlWr0KtXLwDA8uXLkZKSghUrVuDtt99GWloannrqKQQHBwOARcBJS0tDkyZN0LFjR6hUKvj5+RX5WRqNBgMGDMDnn3+O4cOHAwC+/fZb3Lp1C3/5y1/k873yyit48sknAQBPPPFEidcQHx+PTz75xGLb3//+dwwdOlR+PXbsWLzyyisAgISEBGzduhUrVqzA5MmTER8fDx8fH3z88cdQqVQICAjAlStXMGXKFMycORP37t3DX//6V3z88cfyORs1aoT27dsjMzNT/oxJkybh+eefBwDMmTMHLVu2xG+//Vbsz7qi2AOkIN0fPUD5JvYAEdHjy/zL3qygoADz589Hq1atUKdOHdSoUQPJyclIS0sr9jytWrWSvzffart69Wqpj/H29gYA+ZjTp0+jbdu2Fvs/+rosfv/9dxiNRnTo0EHeptPp0LZtW5w6dQoAMHr0aKxduxZt2rTB5MmTsW/fPnnfYcOG4ejRo2jWrBnGjx+P5OTkYj9v0KBB2LVrF65cuQLgQY9W7969Ubt2bQAPbofNmzcPHTp0wKxZs3D8+PESr2HQoEE4evSoxVe/fv0s9gkJCZG/12q1CA4Olq/v1KlTCAkJsZibp0OHDrh79y4uXbqEU6dOITc3F6GhocW2o7ifm72wB0hBWjV7gIioeI46DU7O7Sm/NplMyMrMgouri12XZ3DUaWx2LmdnZ4vXH374IZYuXYq4uDg8+eSTcHZ2RnR0NPLy8oo9z6ODp1UqFUym4v//+edjzL+U/3zMo5PoVeTWn/nYws5p3tarVy9cuHABmzdvxvbt2xEaGooxY8bggw8+wNNPP41z585hy5Yt2L59O1599VX06NED69evL/Tz2rZti0aNGmHt2rUYPXo0vvrqK3k8EQCMGDECPXv2xObNm5GcnIyFCxfiww8/xLhx44q8hpo1a6Jx48Zlvnbz9f35Wgury6OD4otS0s/NHtgDpCC5B4hjgIioCCqVCk4OWosvRweN1TZbf9lzNuo9e/agb9++GDx4MFq3bo0nnngCZ86csdvnFaVZs2Y4cOCAxbZDhw6V+3yNGzeGg4MDvv/+e3mb0WjEoUOH0Lx5c3lb3bp1MWzYMKxevRpxcXEWg7ldXV0xYMAALF++HImJidiwYYM8fqgwERERWLNmDf773/9CrVbLt43MfHx8MGrUKGzcuBFvvfUWli9fXu7rM/vhh4cP7uTn5+Pw4cPyrakWLVpg3759FkFy3759cHFxQf369dGkSRM4Ojri22+/rXA7bI09QArS/jEGKN/OqZaIqDJp3LgxNmzYgH379qF27dpYsmQJMjIyLEKCEsaNG4f/+7//Q3BwMNq3b4/ExEQcP368VGNlHn2aDHjwy3/06NF4++234ebmBl9fXyxevBjZ2dnyOJ2ZM2ciKCgILVu2RG5uLr755hv5upcuXQpvb2+0adMGarUa69atg5eXF2rVqlVkOwYNGoQ5c+Zg/vz56N+/PwwGg/xedHQ0evXqhaZNm+LWrVvYsWNHiTXOzs5GRkaGxTa9Xi/fVgMejAlq0qQJmjdvjqVLl+LWrVvy4PKoqCjExcVh3LhxGDt2LE6fPo1Zs2YhJiYGarUaBoMBU6ZMweTJk+Hg4IAOHTrg2rVrOHHihDx2SRQGIAVp1Q+fAius25CI6HE0Y8YMnDt3Dj179oSTkxPefPNNvPTSS7hz546i7Rg0aBDOnj2LSZMmIScnB6+++iqGDRtm1StUmNdee81q27lz5/Dee+/BZDIhMjISWVlZCA4OxrZt2+QA4eDggKlTp+L8+fNwdHREp06dsHbtWgBAjRo1sGjRIpw5cwYajQbPPPMMkpKSir3V2aRJEzzzzDM4ePAg4uLiLN4rKCjAmDFjcOnSJbi6uuK5557D0qVLi72u5cuXW/US9ezZE1u3bpVfv/fee1i0aBFSU1PRqFEj/Oc//5GfZKtfvz6SkpLw9ttvo3Xr1nBzc8Pw4cMxffp0+fgZM2ZAq9Vi5syZuHLlCry9vTFy5Mhi26UElWTLZx8fE5mZmahZsybu3Lkjz4dQHkajEUlJSejduzd0Oh3uZBvReu6DQW5n5veSb4lRxT1aa7If1tp2cnJycO7cOfj7+1v8S/7PTCYTMjMz4erqatcxQNVVWFgYvLy88O9//5u1fsT58+fh7++P1NTUCs2VVJiK1Lq4vzdl+f3NHiAFmW+BAQ/GAdlwzCEREZUgOzsb//jHP9CzZ09oNBp88cUX2L59O1JSUkQ3jQRgAFLQnwOQ0WSCI5iAiIiUolKpkJSUhHnz5iE3NxfNmjXDhg0b0KNHD9FNIwEYgBSk+1M3H58EIyJSlqOjI7Zv3y66GVVGw4YNbTpDeGXDm5wKUqtV+GMqIORzLiAiIiJhGIAUZh74nMcARER/eJz/lU1ka7b6+8IApDBOhkhEZuan6LKzswW3hKjqMM8grtFUbBwtxwApjJMhEpGZRqNBrVq15DWPnJycrOYHM5lMyMvLQ05ODh/NtjPWWjnlrbXJZMK1a9fg5OQErbZiEYYBSGF/ngyRiMjLywtA0Qs/SpKE+/fvw9HRkZOn2hlrrZyK1FqtVsPX17fCPyMGIIXpzD1ADEBEhAePZnt7e8PDwwNGo9HqfaPRiO+++w6dO3fmxJN2xlorpyK1dnBwsEkPHQOQwjgImogKo9FoCh3ToNFokJ+fD4PBwF/KdsZaK6cy1Jo3ORUmjwFiACIiIhKGAUhh5skQ8028BUZERCQKA5DCzD1ARvYAERERCcMApDAt5wEiIiISjgFIYTo1e4CIiIhEYwBSmPkpMCPHABEREQnDAKQwPgVGREQkHgOQwrgWGBERkXgMQArTmscAcS0wIiIiYRiAFMYeICIiIvEYgBSm4zxAREREwjEAKcw8DxBXgyciIhKHAUhhOj4FRkREJBwDkMK0as4DREREJBoDkMI4DxAREZF4DEAKc5DHADEAERERicIApLCHq8HzFhgREZEowgNQfHw8/P39YTAYEBQUhD179hS5765du6BSqay+fvnlF4v9NmzYgBYtWkCv16NFixb46quv7H0ZpWYeA5TPiRCJiIiEERqAEhMTER0djWnTpiE1NRWdOnVCr169kJaWVuxxp0+fRnp6uvzVpEkT+b39+/djwIABiIyMxLFjxxAZGYlXX30VP/74o70vp1QePgXGHiAiIiJRhAagJUuWYPjw4RgxYgSaN2+OuLg4+Pj4ICEhodjjPDw84OXlJX9pNBr5vbi4OISFhWHq1KkICAjA1KlTERoairi4ODtfTelwHiAiIiLxtKI+OC8vD4cPH0ZsbKzF9vDwcOzbt6/YY5966ink5OSgRYsWmD59Orp16ya/t3//fkycONFi/549exYbgHJzc5Gbmyu/zszMBAAYjUYYjcbSXpIV87F/PocaD4JPrjG/QucmS4XVmuyDtVYW660c1lo59qp1Wc4nLABdv34dBQUF8PT0tNju6emJjIyMQo/x9vbGsmXLEBQUhNzcXPz73/9GaGgodu3ahc6dOwMAMjIyynROAFi4cCHmzJljtT05ORlOTk5lvTQrKSkp8ve/ZqgAaHDp8hUkJV2q8LnJ0p9rTfbFWiuL9VYOa60cW9c6Ozu71PsKC0BmKpXK4rUkSVbbzJo1a4ZmzZrJr0NCQnDx4kV88MEHcgAq6zkBYOrUqYiJiZFfZ2ZmwsfHB+Hh4XB1dS3T9fyZ0WhESkoKwsLCoNPpHpz74CWsP3cS7h6e6N37qXKfmywVVmuyD9ZaWay3clhr5dir1uY7OKUhLAC5u7tDo9FY9cxcvXrVqgenOM8++yxWr14tv/by8irzOfV6PfR6vdV2nU5nkx/Mn8+jd3hQ8gIJ/AtmB7b6mVHJWGtlsd7KYa2VY+tal+VcwgZBOzg4ICgoyKr7KyUlBe3bty/1eVJTU+Ht7S2/DgkJsTpncnJymc5pT/JTYFwKg4iISBiht8BiYmIQGRmJ4OBghISEYNmyZUhLS8OoUaMAPLg1dfnyZXz22WcAHjzh1bBhQ7Rs2RJ5eXlYvXo1NmzYgA0bNsjnnDBhAjp37oxFixahb9+++M9//oPt27fj+++/F3KNj5LXAuNM0ERERMIIDUADBgzAjRs3MHfuXKSnpyMwMBBJSUnw8/MDAKSnp1vMCZSXl4dJkybh8uXLcHR0RMuWLbF582b07t1b3qd9+/ZYu3Ytpk+fjhkzZqBRo0ZITExEu3btFL++wuj4GDwREZFwwgdBR0VFISoqqtD3Vq1aZfF68uTJmDx5conn7N+/P/r372+L5tmcjouhEhERCSd8KYzqhhMhEhERiccApDCd2jwImj1AREREojAAKczcA8S1wIiIiMRhAFKY9o8xQHkcA0RERCQMA5DCHNgDREREJBwDkMK0Go4BIiIiEo0BSGEPJ0JkDxAREZEoDEAK4zxARERE4jEAKYzzABEREYnHAKQwcw+QkWOAiIiIhGEAUpjujzFAkgQUcEV4IiIiIRiAFGZ+CgzgivBERESiMAApzLwaPADksweIiIhICAYghWnVD3uA+CQYERGRGAxACtOoVVD9kYG4HAYREZEYDEAKU6lU8kBoLodBREQkBgOQAPJyGAxAREREQjAACWAeB8S5gIiIiMRgABJAxxXhiYiIhGIAEsB8C4zzABEREYnBACSATl4PjAGIiIhIBAYgAeRbYJwIkYiISAgGIAHkQdDsASIiIhKCAUgALQdBExERCcUAJICOg6CJiIiEYgAS4OEgaPYAERERicAAJIB5DFA+J0IkIiISggFIAE6ESEREJBYDkACcCJGIiEgsBiABtGrOA0RERCQSA5AAfAqMiIhILAYgAfgUGBERkVgMQAKYxwDlsweIiIhICAYgAXQcA0RERCQUA5AAfAqMiIhILAYgAR6OAWIAIiIiEoEBSACdPAaIt8CIiIhEYAASQMunwIiIiIRiABJAx7XAiIiIhGIAEoA9QERERGIxAAnAp8CIiIjEYgASwEFeDZ4BiIiISAQGIAG0f4wBMnIiRCIiIiEYgATQsgeIiIhIKAYgATgPEBERkVgMQAJo/1gLjLfAiIiIxGAAEkB+Ciyft8CIiIhEYAASQH4KjBMhEhERCcEAJAAnQiQiIhKLAUgA8y0w9gARERGJITwAxcfHw9/fHwaDAUFBQdizZ0+pjtu7dy+0Wi3atGlj9V5cXByaNWsGR0dH+Pj4YOLEicjJybFxy8tPpzY/Bs8eICIiIhGEBqDExERER0dj2rRpSE1NRadOndCrVy+kpaUVe9ydO3cwZMgQhIaGWr23Zs0axMbGYtasWTh16hRWrFiBxMRETJ061V6XUWbmHqA8zgNEREQkhNAAtGTJEgwfPhwjRoxA8+bNERcXBx8fHyQkJBR73MiRIxEREYGQkBCr9/bv348OHTogIiICDRs2RHh4OAYOHIhDhw7Z6zLKTKdhDxAREZFIwgJQXl4eDh8+jPDwcIvt4eHh2LdvX5HHrVy5Er///jtmzZpV6PsdO3bE4cOHceDAAQDA2bNnkZSUhOeff952ja+ghxMhsgeIiIhIBK2oD75+/ToKCgrg6elpsd3T0xMZGRmFHnPmzBnExsZiz5490GoLb/prr72Ga9euoWPHjpAkCfn5+Rg9ejRiY2OLbEtubi5yc3Pl15mZmQAAo9EIo9FY1kuTmY+1Oscfg5+NBaYKnZ8eKrLWZHOstbJYb+Ww1sqxV63Lcj5hAchMpVJZvJYkyWobABQUFCAiIgJz5sxB06ZNizzfrl27MH/+fMTHx6Ndu3b47bffMGHCBHh7e2PGjBmFHrNw4ULMmTPHantycjKcnJzKeEXWUlJSLF5nZAOAFtk5uUhKSqrw+emhR2tN9sNaK4v1Vg5rrRxb1zo7O7vU+6okSRIyECUvLw9OTk5Yt24d+vXrJ2+fMGECjh49it27d1vsf/v2bdSuXRsajUbeZjKZIEkSNBoNkpOT0b17d3Tq1AnPPvss3n//fXm/1atX480338Tdu3ehVlvf9SusB8jHxwfXr1+Hq6trua/RaDQiJSUFYWFh0Ol08vbzN+4hLG4vnPUaHJ1uPZCbyq6oWpPtsdbKYr2Vw1orx161zszMhLu7O+7cuVPi729hPUAODg4ICgpCSkqKRQBKSUlB3759rfZ3dXXFiRMnLLbFx8djx44dWL9+Pfz9/QE8SH+PhhyNRgNJklBU1tPr9dDr9VbbdTqdTX4wj57H4OAA4MEgaP4lsy1b/cyoZKy1slhv5bDWyrF1rctyLqG3wGJiYhAZGYng4GCEhIRg2bJlSEtLw6hRowAAU6dOxeXLl/HZZ59BrVYjMDDQ4ngPDw8YDAaL7X369MGSJUvw1FNPybfAZsyYgRdffNGi90gkB615KQw+BUZERCSC0AA0YMAA3LhxA3PnzkV6ejoCAwORlJQEPz8/AEB6enqJcwI9avr06VCpVJg+fTouX76MunXrok+fPpg/f749LqFctOoHY5wKTFKRY56IiIjIfoQPgo6KikJUVFSh761atarYY2fPno3Zs2dbbNNqtZg1a1aRj8lXBua1wIAH64E5aBmAiIiIlCR8KYzqyDwPEMD1wIiIiERgABJA+6dB2sZ8jgMiIiJSGgOQAH/uATKyB4iIiEhxDEACqFQqeSA01wMjIiJSHgOQIOYV4Y1cD4yIiEhxDECC6NScC4iIiEgUBiBB2ANEREQkDgOQILo/5gJiACIiIlIeA5Ag5gDEQdBERETKYwASxHwLjBMhEhERKY8BSBDzY/BG9gAREREpjgFIEI4BIiIiEocBSBD5Fhh7gIiIiBTHACQIe4CIiIjEYQAShBMhEhERicMAJAgnQiQiIhKHAUgQLecBIiIiEoYBSBCdmj1AREREojAACSIPguYYICIiIsUxAAny8DF49gAREREpjQFIEK4FRkREJA4DkCDyUhhcC4yIiEhxDECCmJ8CM+azB4iIiEhpDECCOHA1eCIiImEYgASRe4A4BoiIiEhxDECC8CkwIiIicRiABOFaYEREROIwAAli7gHKYw8QERGR4hiABHk4DxADEBERkdIYgATRyWOAeAuMiIhIaQxAgmjVXAuMiIhIFAYgQXR8CoyIiEgYBiBBOA8QERGROAxAgshrgbEHiIiISHEMQII4aM3zADEAERERKY0BSBB5EDRvgRERESmOAUgQLoVBREQkDgOQIPJTYHwMnoiISHEMQIKYb4Hl5bMHiIiISGkMQIJo2QNEREQkDAOQIA5cC4yIiEgYBiBBOBEiERGROAxAgpgnQuQ8QERERMpjABJExx4gIiIiYRiABDEPguZSGERERMpjABLk4SBo9gAREREpjQFIkIePwbMHiIiISGkMQIL8eS0wSWIvEBERkZIYgAQxL4UBAAWcDJGIiEhR5QpAFy9exKVLl+TXBw4cQHR0NJYtW2azhj3uzPMAAXwSjIiISGnlCkARERHYuXMnACAjIwNhYWE4cOAA3nnnHcydO9emDXxc/bkHyMhxQERERIoqVwD66aef0LZtWwDAl19+icDAQOzbtw+ff/45Vq1aVaZzxcfHw9/fHwaDAUFBQdizZ0+pjtu7dy+0Wi3atGlj9d7t27cxZswYeHt7w2AwoHnz5khKSipTu+xNp35Yej4JRkREpCxteQ4yGo3Q6/UAgO3bt+PFF18EAAQEBCA9Pb3U50lMTER0dDTi4+PRoUMH/POf/0SvXr1w8uRJ+Pr6FnncnTt3MGTIEISGhuJ///ufxXt5eXkICwuDh4cH1q9fjwYNGuDixYtwcXEpx5Xaj1qtgloFmCSuB0ZERKS0cvUAtWzZEv/4xz+wZ88epKSk4LnnngMAXLlyBXXq1Cn1eZYsWYLhw4djxIgRaN68OeLi4uDj44OEhIRijxs5ciQiIiIQEhJi9d6nn36Kmzdv4uuvv0aHDh3g5+eHjh07onXr1mW7SAXI64FxEDQREZGiyhWAFi1ahH/+85/o2rUrBg4cKIeLTZs2ybfGSpKXl4fDhw8jPDzcYnt4eDj27dtX5HErV67E77//jlmzZhX6/qZNmxASEoIxY8bA09MTgYGBWLBgAQoKCkp5dcrR/bEemDGfPUBERERKKtctsK5du+L69evIzMxE7dq15e1vvvkmnJycSnWO69evo6CgAJ6enhbbPT09kZGRUegxZ86cQWxsLPbs2QOttvCmnz17Fjt27MCgQYOQlJSEM2fOYMyYMcjPz8fMmTMLPSY3Nxe5ubny68zMTAAPbvUZjcZSXU9hzMcWdQ7zZIj3c/NgNDqU+3Oo5FqT7bDWymK9lcNaK8detS7L+coVgO7fvw9JkuTwc+HCBXz11Vdo3rw5evbsWaZzqVQqi9eSJFltA4CCggJERERgzpw5aNq0aZHnM5lM8PDwwLJly6DRaBAUFIQrV67g/fffLzIALVy4EHPmzLHanpycXOpAV5yUlJTC25qvAaDCzt3f4VfnCn8Moehak+2x1spivZXDWivH1rXOzs4u9b7lCkB9+/bFyy+/jFGjRuH27dto164ddDodrl+/jiVLlmD06NElnsPd3R0ajcaqt+fq1atWvUIAkJWVhUOHDiE1NRVjx44F8CDsSJIErVaL5ORkdO/eHd7e3tDpdNBoNPKxzZs3R0ZGBvLy8uDgYN3TMnXqVMTExMivMzMz4ePjg/DwcLi6upa6Lo8yGo1ISUlBWFgYdDqd1fsLf96NLGMuQtp3RGD98n8OlVxrsh3WWlmst3JYa+XYq9bmOzilUa4AdOTIESxduhQAsH79enh6eiI1NRUbNmzAzJkzSxWAHBwcEBQUhJSUFPTr10/enpKSgr59+1rt7+rqihMnTlhsi4+Px44dO7B+/Xr4+/sDADp06IDPP/8cJpMJ6j8eNf/111/h7e1daPgBAL1eLz/V9mc6nc4mP5iizqPTPmifpFbzL5uN2OpnRiVjrZXFeiuHtVaOrWtdlnOVKwBlZ2fLj5UnJyfj5ZdfhlqtxrPPPosLFy6U+jwxMTGIjIxEcHAwQkJCsGzZMqSlpWHUqFEAHvTMXL58GZ999hnUajUCAwMtjvfw8IDBYLDYPnr0aPztb3/DhAkTMG7cOJw5cwYLFizA+PHjy3OpdmWeC4jzABERESmrXAGocePG+Prrr9GvXz9s27YNEydOBPDg9lVZbhkNGDAAN27cwNy5c5Geno7AwEAkJSXBz88PAJCeno60tLQytc3HxwfJycmYOHEiWrVqhfr162PChAmYMmVKmc6jBPMgaCPnASIiIlJUuQLQzJkzERERgYkTJ6J79+7yfDzJycl46qmnynSuqKgoREVFFfpeSbNKz549G7Nnz7baHhISgh9++KFM7RBBZ54HiAGIiIhIUeUKQP3790fHjh2Rnp5uMcFgaGioxXgeKp55IkTeAiMiIlJWuQIQAHh5ecHLywuXLl2CSqVC/fr1Sz0JIj1gnggxn4uhEhERKapcM0GbTCbMnTsXNWvWhJ+fH3x9fVGrVi28++67MPGXeak9HAPEHiAiIiIllasHaNq0aVixYgXee+89dOjQAZIkYe/evZg9ezZycnIwf/58W7fzscQxQERERGKUKwD961//wieffCKvAg8ArVu3Rv369REVFcUAVEpa8y0w9gAREREpqly3wG7evImAgACr7QEBAbh582aFG1VdyD1AvG1IRESkqHIFoNatW+Pjjz+22v7xxx+jVatWFW5UdaHjU2BERERClOsW2OLFi/H8889j+/btCAkJgUqlwr59+3Dx4kUkJSXZuo2PLU6ESEREJEa5eoC6dOmCX3/9Ff369cPt27dx8+ZNvPzyy/j555+xcuVKW7fxsaVVmwdBsweIiIhISeWeB6hevXpWg52PHTuGf/3rX/j0008r3LDqQKcxD4JmDxAREZGSytUDRLbxcBA0e4CIiIiUxAAkkJY9QEREREIwAAkkPwXGHiAiIiJFlWkM0Msvv1zs+7dv365IW6od80SIfAqMiIhIWWUKQDVr1izx/SFDhlSoQdWJlkthEBERCVGmAMRH3G3LQcOlMIiIiETgGCCBHvYAMQAREREpiQFIIHkxVK4FRkREpCgGIIG4FhgREZEYDEACmecByuMgaCIiIkUxAAmkU5t7gBiAiIiIlMQAJJBOax4DxFtgRERESmIAEujhavDsASIiIlISA5BAOs4DREREJAQDkEDsASIiIhKDAUgg81NgnAiRiIhIWQxAAjnIq8GzB4iIiEhJDEACaTkRIhERkRAMQALJt8DYA0RERKQoBiCBHk6EyB4gIiIiJTEACfRwEDR7gIiIiJTEACSQeTFUPgVGRESkLAYggR5OhMgeICIiIiUxAAlkfgrMyLXAiIiIFMUAJJBOzR4gIiIiERiABDL3AJkkoIC9QERERIphABLI/BQYwCfBiIiIlMQAJJB5KQwAyGcPEBERkWIYgATSqh/2AHEcEBERkXIYgATSqP98C4w9QEREREphABJIpVLJcwFxDBAREZFyGIAE03I9MCIiIsUxAAmm44rwREREimMAEsy8Hhh7gIiIiJTDACQYV4QnIiJSHgOQYPIYIM4DREREpBgGIMH4FBgREZHyGIAEk1eEZwAiIiJSDAOQYBwETUREpDwGIMHMt8Dy+Rg8ERGRYhiABDOvB8alMIiIiJQjPADFx8fD398fBoMBQUFB2LNnT6mO27t3L7RaLdq0aVPkPmvXroVKpcJLL71km8baAccAERERKU9oAEpMTER0dDSmTZuG1NRUdOrUCb169UJaWlqxx925cwdDhgxBaGhokftcuHABkyZNQqdOnWzdbJuSb4GxB4iIiEgxQgPQkiVLMHz4cIwYMQLNmzdHXFwcfHx8kJCQUOxxI0eOREREBEJCQgp9v6CgAIMGDcKcOXPwxBNP2KPpNqNjDxAREZHitKI+OC8vD4cPH0ZsbKzF9vDwcOzbt6/I41auXInff/8dq1evxrx58wrdZ+7cuahbty6GDx9eqltqubm5yM3NlV9nZmYCAIxGI4xGY2kup1DmY4s7xx8dQMg15lfos6q70tSabIO1VhbrrRzWWjn2qnVZzicsAF2/fh0FBQXw9PS02O7p6YmMjIxCjzlz5gxiY2OxZ88eaLWFN33v3r1YsWIFjh49Wuq2LFy4EHPmzLHanpycDCcnp1KfpygpKSlFvnfjqhqAGqnHTsDl6vEKf1Z1V1ytybZYa2Wx3sphrZVj61pnZ2eXel9hAchMpVJZvJYkyWob8OC2VkREBObMmYOmTZsWeq6srCwMHjwYy5cvh7u7e6nbMHXqVMTExMivMzMz4ePjg/DwcLi6upb6PI8yGo1ISUlBWFgYdDpdofvsM57E0ZuXUM+/CXp3b1zuz6ruSlNrsg3WWlmst3JYa+XYq9bmOzilISwAubu7Q6PRWPX2XL161apXCHgQbg4dOoTU1FSMHTsWAGAymSBJErRaLZKTk+Hm5obz58+jT58+8nGmP+bX0Wq1OH36NBo1amR1br1eD71eb7Vdp9PZ5AdT3Hk8azoCAG5m5/MvnA3Y6mdGJWOtlcV6K4e1Vo6ta12WcwkLQA4ODggKCkJKSgr69esnb09JSUHfvn2t9nd1dcWJEycstsXHx2PHjh1Yv349/P39odForPaZPn06srKy8Ne//hU+Pj72uZgKqFvDAQBw/W5uCXsSERGRrQi9BRYTE4PIyEgEBwcjJCQEy5YtQ1paGkaNGgXgwa2py5cv47PPPoNarUZgYKDF8R4eHjAYDBbbH92nVq1ahW6vLNxrPOh5upbFAERERKQUoQFowIABuHHjBubOnYv09HQEBgYiKSkJfn5+AID09PQS5wSq6uq6PAhA1+/mCW4JERFR9SF8EHRUVBSioqIKfW/VqlXFHjt79mzMnj272H1KOodo5h4g3gIjIiJSjvClMKo79z96gLLzCnAvN19wa4iIiKoHBiDBnB00cNRpALAXiIiISCkMQIKpVCq4u/BJMCIiIiUxAFUCfBKMiIhIWQxAlYAcgPgkGBERkSIYgCoB+VF49gAREREpggGoEuCj8ERERMpiAKoEzMthcAwQERGRMhiAKoGHs0EzABERESmBAagSeHgLjIOgiYiIlMAAVAnwMXgiIiJlMQBVAuZbYPeNXA6DiIhICQxAlYCzXsvlMIiIiBTEAFRJmJfD4G0wIiIi+2MAqiTqci4gIiIixTAAVRJcDoOIiEg5DECVhLsLnwQjIiJSCgNQJcFbYERERMphAKok3LkgKhERkWIYgCoJeT0w9gARERHZHQNQJcEV4YmIiJTDAFRJyAuiZvEpMCIiIntjAKokzD1AXA6DiIjI/hiAKok/L4fBR+GJiIjsiwGoEpFvg3EcEBERkV0xAFUi7n88CcYAREREZF8MQJWIvBwGb4ERERHZFQNQJWK+Bcb1wIiIiOyLAagS4VxAREREymAAqkS4ICoREZEyGIAqES6ISkREpAwGoEqkrgufAiMiIlICA1Al8uenwCRJEtwaIiKixxcDUCViDkA5RhPu5RUIbg0REdHjiwGoEnHWa+Hk8GA5jOscCE1ERGQ3DECVjHwbjOOAiIiI7IYBqJKR1wNjDxAREZHdMABVMlwPjIiIyP4YgCqZh7fAuBwGERGRvTAAVTJcEJWIiMj+GIAqGXkMEG+BERER2Q0DUCXDBVGJiIjsjwGokjEvh8FbYERERPbDAFTJ1K1hAPAgAJlMXA6DiIjIHhiAKpl6tQzQa9XIzTfh/I17optDRET0WGIAqmS0GjUCvF0BAD9fyRTcGiIioscTA1AlFFiPAYiIiMieGIAqoZb1agIAfr5yR3BLiIiIHk8MQJVQyz96gE5eyYQkcSA0ERGRrTEAVULNvFygUatw414eMjJzRDeHiIjoscMAVAkZdBo0rlsDAPDzZY4DIiIisjXhASg+Ph7+/v4wGAwICgrCnj17SnXc3r17odVq0aZNG4vty5cvR6dOnVC7dm3Url0bPXr0wIEDB+zQcvtqyYHQREREdiM0ACUmJiI6OhrTpk1DamoqOnXqhF69eiEtLa3Y4+7cuYMhQ4YgNDTU6r1du3Zh4MCB2LlzJ/bv3w9fX1+Eh4fj8uXL9roMu2ghByAOhCYiIrI1oQFoyZIlGD58OEaMGIHmzZsjLi4OPj4+SEhIKPa4kSNHIiIiAiEhIVbvrVmzBlFRUWjTpg0CAgKwfPlymEwmfPvtt/a6DLt4+CQYe4CIiIhsTSvqg/Py8nD48GHExsZabA8PD8e+ffuKPG7lypX4/fffsXr1asybN6/Ez8nOzobRaISbm1uR++Tm5iI39+HaW5mZD0KH0WiE0Wgs8TOKYj62POdoWtcRAHD59n1cu5ONWk66crejOqhIralsWGtlsd7KYa2VY69al+V8wgLQ9evXUVBQAE9PT4vtnp6eyMjIKPSYM2fOIDY2Fnv27IFWW7qmx8bGon79+ujRo0eR+yxcuBBz5syx2p6cnAwnJ6dSfU5xUlJSynVcHb0GN3JVWPWf7Whak4/Dl0Z5a01lx1ori/VWDmutHFvXOjs7u9T7CgtAZiqVyuK1JElW2wCgoKAAERERmDNnDpo2bVqqcy9evBhffPEFdu3aBYPBUOR+U6dORUxMjPw6MzMTPj4+CA8Ph6uraymvxJrRaERKSgrCwsKg05W9B2fznaNIPnkVLr7N0btDw3K3ozqoaK2p9FhrZbHeymGtlWOvWpvv4JSGsADk7u4OjUZj1dtz9epVq14hAMjKysKhQ4eQmpqKsWPHAgBMJhMkSYJWq0VycjK6d+8u7//BBx9gwYIF2L59O1q1alVsW/R6PfR6vdV2nU5nkx9Mec/zZP1aSD55Fb9k3OVfxlKy1c+MSsZaK4v1Vg5rrRxb17os5xIWgBwcHBAUFISUlBT069dP3p6SkoK+ffta7e/q6ooTJ05YbIuPj8eOHTuwfv16+Pv7y9vff/99zJs3D9u2bUNwcLD9LsLOWtbno/BERET2IPQWWExMDCIjIxEcHIyQkBAsW7YMaWlpGDVqFIAHt6YuX76Mzz77DGq1GoGBgRbHe3h4wGAwWGxfvHgxZsyYgc8//xwNGzaUe5hq1KiBGjVqKHdxNmB+Euz3a3dxP68Ajg4awS0iIiJ6PAgNQAMGDMCNGzcwd+5cpKenIzAwEElJSfDz8wMApKenlzgn0KPi4+ORl5eH/v37W2yfNWsWZs+ebaumK8LDRQ/3Gg64fjcPv2Rk4inf2qKbRERE9FgQPgg6KioKUVFRhb63atWqYo+dPXu2Vag5f/68bRpWCahUKrSoVxPf/XoNP19hACIiIrIV4UthUPG4JAYREZHtMQBVcuYAdJJLYhAREdkMA1AlZx4I/UtGFvILTIJbQ0RE9HhgAKrk/NycUEOvRW6+Cb9fuye6OURERI8FBqBKTq1Wobm3CwCuDE9ERGQrDEBVgPk22PFLDEBERES2wABUBbTxqQUAOHrxttB2EBERPS4YgKqAp3xrAQBOXslEbn6B2MYQERE9BhiAqgBfNye4OTsgr8CEk5wPiIiIqMIYgKoAlUqFp/64DZaadltoW4iIiB4HDEBVhHkcUCrHAREREVUYA1AVYV4HLDXtluCWEBERVX0MQFVEK5+aUKmAS7fu41pWrujmEBERVWkMQFWEq0GHJh41APBxeCIioopiAKpCnvLhbTAiIiJbYACqQszzAfFJMCIioophAKpCzAOhj1+6jQKTJLg1REREVRcDUBXS2KMGnB00uJdXgDNXs0Q3h4iIqMpiAKpCNGoVWnNCRCIiogpjAKpiHo4D4kBoIiKi8mIAqmIePgl2W2xDiIiIqjAGoCqmzR89QL9du4vMHKPYxhAREVVRDEBVjHsNPXzdnCBJwPGLd0Q3h4iIqEpiAKqC5IVROQ6IiIioXBiAqiB5IDSXxCAiIioXBqAq6M8rw0sSJ0QkIiIqKwagKqiFtyucHTS4lW3E5wfSRDeHiIioymEAqoIctGq8Fd4MADB/8ylcvJktuEVERERVCwNQFTWsfUO0beiG7LwCvL3+GExcG4yIiKjUGICqKLVahff/0gqOOg1+OHsT//7hgugmERERVRkMQFWYXx1nvNM7AADw3pZfcP76PcEtIiIiqhoYgKq4Qe380L5RHdw3FmDSumMo4K0wIiKiEjEAVXFqtQqLXmkFZwcNDl24hb/v/E10k4iIiCo9BqDHgI+bE2b2aQEAWJLyK1buPSe4RURERJUbA9BjYsAzvhjfvTEAYM5/T+LzHzk/EBERUVEYgB4jE8OaYmTnJwAA074+gQ2HLwluERERUeXEAPQYUalUiO0VgGHtG0KSgLfXH8N/j10R3SwiIqJKhwHoMaNSqTDzhRZ47RkfmCRg3BepGP9FKh+RJyIi+hMGoMeQWq3Cgn5PIvJZPwDApmNX0GPJbrzz1Qn8LzNHcOuIiIjEYwB6TKnVKrz7UiC+GdcRXZvVRb5Jwuc/pqHz4p1468tj2PZzBu7nFYhuJhERkRBa0Q0g+wqsXxOrXm+LA+duYvHWX3Dowi1sOHIJG45cgl6rRqcm7ugW4IFmni5oVLcGajs7iG4yERGR3TEAVRNt/d2wblQIDp6/ha0/ZSD5ZAYu3bqP7aeuYvupq/J+bs4OeMLdGZ6uBrg6auFq0MHVUYcaei0MOjX0Wo38X51GDY1aBZ1GBa1GDa1aBZUKUKtUUKvM3wPAg+9VeDBGSfXHZz3YppK/L0lx++Tn5+NmLnD59n1otcbylolKgbVWFuutHNZaOfn5+cjME9sGlSRJXDvhEZmZmahZsybu3LkDV1fXcp/HaDQiKSkJvXv3hk6ns2ELK06SJPySkYXkn/+HQxdu4uy1e7h8+77oZhERUTXRsIaElCk9bfr7sSy/v9kDVE2pVCo093ZFc++Hf0Cy8/Jx9to9nL1+D7fu5SHzvhGZOUZk3s/H3dx85OYXIMdokv9rLDAh3yQhX/6vBJMkwSQ9CFgSgAKTJH8PCTCn7T/n7ofbzK9LzuSFxXZTQQHUGk15ykFlxFori/VWDmutHK1a7DhUBiCSOTloEVi/JgLr1xTdlDJ72Ntm239NkDXWWlmst3JYa+WYay0SnwIjIiKiaocBiIiIiKodBiAiIiKqdhiAiIiIqNphACIiIqJqhwGIiIiIqh3hASg+Ph7+/v4wGAwICgrCnj17SnXc3r17odVq0aZNG6v3NmzYgBYtWkCv16NFixb46quvbNxqIiIiqsqEBqDExERER0dj2rRpSE1NRadOndCrVy+kpaUVe9ydO3cwZMgQhIaGWr23f/9+DBgwAJGRkTh27BgiIyPx6quv4scff7TXZRAREVEVIzQALVmyBMOHD8eIESPQvHlzxMXFwcfHBwkJCcUeN3LkSERERCAkJMTqvbi4OISFhWHq1KkICAjA1KlTERoairi4ODtdBREREVU1wmaCzsvLw+HDhxEbG2uxPTw8HPv27SvyuJUrV+L333/H6tWrMW/ePKv39+/fj4kTJ1ps69mzZ7EBKDc3F7m5ufLrzMxMAA9mqjQay78gnvnYipyDSoe1Vg5rrSzWWzmstXLsVeuynE9YALp+/ToKCgrg6elpsd3T0xMZGRmFHnPmzBnExsZiz5490GoLb3pGRkaZzgkACxcuxJw5c6y2Jycnw8nJqaRLKVFKSkqFz0Glw1orh7VWFuutHNZaObaudXZ2dqn3Fb4WmEqlsngtSZLVNgAoKChAREQE5syZg6ZNm9rknGZTp05FTEyM/DozMxM+Pj4IDw+v8GrwKSkpCAsL47oydsZaK4e1VhbrrRzWWjn2qrX5Dk5pCAtA7u7u0Gg0Vj0zV69eterBAYCsrCwcOnQIqampGDt2LADAZDJBkiRotVokJyeje/fu8PLyKvU5zfR6PfR6vdV2nU5nkx+Mrc5DJWOtlcNaK4v1Vg5rrRxb17os5xI2CNrBwQFBQUFW3V8pKSlo37691f6urq44ceIEjh49Kn+NGjUKzZo1w9GjR9GuXTsAQEhIiNU5k5OTCz0nERERVU9Cb4HFxMQgMjISwcHBCAkJwbJly5CWloZRo0YBeHBr6vLly/jss8+gVqsRGBhocbyHhwcMBoPF9gkTJqBz585YtGgR+vbti//85z/Yvn07vv/++1K3S5IkAGXrSiuM0WhEdnY2MjMz+a8JO2OtlcNaK4v1Vg5rrRx71dr8e9v8e7w4QgPQgAEDcOPGDcydOxfp6ekIDAxEUlIS/Pz8AADp6eklzgn0qPbt22Pt2rWYPn06ZsyYgUaNGiExMVHuISqNrKwsAICPj0+ZPpuIiIjEy8rKQs2aNYvdRyWVJiZVMyaTCVeuXIGLi0uxg6dLYh5MffHixQoNpqaSsdbKYa2VxXorh7VWjr1qLUkSsrKyUK9ePajVxY/yEf4UWGWkVqvRoEEDm53P1dWVf5kUwlorh7VWFuutHNZaOfaodUk9P2bC1wIjIiIiUhoDEBEREVU7DEB2pNfrMWvWrELnGCLbYq2Vw1ori/VWDmutnMpQaw6CJiIiomqHPUBERERU7TAAERERUbXDAERERETVDgMQERERVTsMQHYUHx8Pf39/GAwGBAUFYc+ePaKbVKUtXLgQzzzzDFxcXODh4YGXXnoJp0+ftthHkiTMnj0b9erVg6OjI7p27Yqff/5ZUIsfHwsXLoRKpUJ0dLS8jbW2rcuXL2Pw4MGoU6cOnJyc0KZNGxw+fFh+n/W2jfz8fEyfPh3+/v5wdHTEE088gblz58JkMsn7sNbl891336FPnz6oV68eVCoVvv76a4v3S1PX3NxcjBs3Du7u7nB2dsaLL76IS5cu2afBEtnF2rVrJZ1OJy1fvlw6efKkNGHCBMnZ2Vm6cOGC6KZVWT179pRWrlwp/fTTT9LRo0el559/XvL19ZXu3r0r7/Pee+9JLi4u0oYNG6QTJ05IAwYMkLy9vaXMzEyBLa/aDhw4IDVs2FBq1aqVNGHCBHk7a207N2/elPz8/KRhw4ZJP/74o3Tu3Dlp+/bt0m+//Sbvw3rbxrx586Q6depI33zzjXTu3Dlp3bp1Uo0aNaS4uDh5H9a6fJKSkqRp06ZJGzZskABIX331lcX7panrqFGjpPr160spKSnSkSNHpG7dukmtW7eW8vPzbd5eBiA7adu2rTRq1CiLbQEBAVJsbKygFj1+rl69KgGQdu/eLUmSJJlMJsnLy0t677335H1ycnKkmjVrSv/4xz9ENbNKy8rKkpo0aSKlpKRIXbp0kQMQa21bU6ZMkTp27Fjk+6y37Tz//PPSG2+8YbHt5ZdflgYPHixJEmttK48GoNLU9fbt25JOp5PWrl0r73P58mVJrVZLW7dutXkbeQvMDvLy8nD48GGEh4dbbA8PD8e+ffsEterxc+fOHQCAm5sbAODcuXPIyMiwqLter0eXLl1Y93IaM2YMnn/+efTo0cNiO2ttW5s2bUJwcDD+8pe/wMPDA0899RSWL18uv896207Hjh3x7bff4tdffwUAHDt2DN9//z169+4NgLW2l9LU9fDhwzAajRb71KtXD4GBgXapPRdDtYPr16+joKAAnp6eFts9PT2RkZEhqFWPF0mSEBMTg44dOyIwMBAA5NoWVvcLFy4o3saqbu3atThy5AgOHjxo9R5rbVtnz55FQkICYmJi8M477+DAgQMYP3489Ho9hgwZwnrb0JQpU3Dnzh0EBARAo9GgoKAA8+fPx8CBAwHwz7a9lKauGRkZcHBwQO3ata32scfvTgYgO1KpVBavJUmy2kblM3bsWBw/fhzff/+91Xuse8VdvHgREyZMQHJyMgwGQ5H7sda2YTKZEBwcjAULFgAAnnrqKfz8889ISEjAkCFD5P1Y74pLTEzE6tWr8fnnn6Nly5Y4evQooqOjUa9ePQwdOlTej7W2j/LU1V615y0wO3B3d4dGo7FKrFevXrVKv1R248aNw6ZNm7Bz5040aNBA3u7l5QUArLsNHD58GFevXkVQUBC0Wi20Wi12796Njz76CFqtVq4na20b3t7eaNGihcW25s2bIy0tDQD/bNvS22+/jdjYWLz22mt48sknERkZiYkTJ2LhwoUAWGt7KU1dvby8kJeXh1u3bhW5jy0xANmBg4MDgoKCkJKSYrE9JSUF7du3F9Sqqk+SJIwdOxYbN27Ejh074O/vb/G+v78/vLy8LOqel5eH3bt3s+5lFBoaihMnTuDo0aPyV3BwMAYNGoSjR4/iiSeeYK1tqEOHDlZTOvz666/w8/MDwD/btpSdnQ212vJXn0ajkR+DZ63tozR1DQoKgk6ns9gnPT0dP/30k31qb/Nh1SRJ0sPH4FesWCGdPHlSio6OlpydnaXz58+LblqVNXr0aKlmzZrSrl27pPT0dPkrOztb3ue9996TatasKW3cuFE6ceKENHDgQD6+aiN/fgpMklhrWzpw4ICk1Wql+fPnS2fOnJHWrFkjOTk5SatXr5b3Yb1tY+jQoVL9+vXlx+A3btwoubu7S5MnT5b3Ya3LJysrS0pNTZVSU1MlANKSJUuk1NRUefqX0tR11KhRUoMGDaTt27dLR44ckbp3787H4Kuiv//975Kfn5/k4OAgPf300/Lj2lQ+AAr9WrlypbyPyWSSZs2aJXl5eUl6vV7q3LmzdOLECXGNfow8GoBYa9v673//KwUGBkp6vV4KCAiQli1bZvE+620bmZmZ0oQJEyRfX1/JYDBITzzxhDRt2jQpNzdX3oe1Lp+dO3cW+v/ooUOHSpJUurrev39fGjt2rOTm5iY5OjpKL7zwgpSWlmaX9qokSZJs369EREREVHlxDBARERFVOwxAREREVO0wABEREVG1wwBERERE1Q4DEBEREVU7DEBERERU7TAAERERUbXDAEREVAoqlQpff/216GYQkY0wABFRpTds2DCoVCqrr+eee05004ioitKKbgARUWk899xzWLlypcU2vV4vqDVEVNWxB4iIqgS9Xg8vLy+Lr9q1awN4cHsqISEBvXr1gqOjI/z9/bFu3TqL40+cOIHu3bvD0dERderUwZtvvom7d+9a7PPpp5+iZcuW0Ov18Pb2xtixYy3ev379Ovr16wcnJyc0adIEmzZtsu9FE5HdMAAR0WNhxowZeOWVV3Ds2DEMHjwYAwcOxKlTpwAA2dnZeO6551C7dm0cPHgQ69atw/bt2y0CTkJCAsaMGYM333wTJ06cwKZNm9C4cWOLz5gzZw5effVVHD9+HL1798agQYNw8+ZNRa+TiGzELkusEhHZ0NChQyWNRiM5OztbfM2dO1eSJEkCII0aNcrimHbt2kmjR4+WJEmSli1bJtWuXVu6e/eu/P7mzZsltVotZWRkSJIkSfXq1ZOmTZtWZBsASNOnT5df3717V1KpVNKWLVtsdp1EpByOASKiKqFbt25ISEiw2Obm5iZ/HxISYvFeSEgIjh49CgA4deoUWrduDWdnZ/n9Dh06wGQy4fTp01CpVLhy5QpCQ0OLbUOrVq3k752dneHi4oKrV6+W95KISCAGICKqEpydna1uSZVEpVIBACRJkr8vbB9HR8dSnU+n01kdazKZytQmIqocOAaIiB4LP/zwg9XrgIAAAECLFi1w9OhR3Lt3T35/7969UKvVaNq0KVxcXNCwYUN8++23iraZiMRhDxARVQm5ubnIyMiw2KbVauHu7g4AWLduHYKDg9GxY0esWbMGBw4cwIoVKwAAgwYNwqxZszB06FDMnj0b165dw7hx4xAZGQlPT08AwOzZszFq1Ch4eHigV69eyMrKwt69ezFu3DhlL5SIFMEARERVwtatW+Ht7W2xrVmzZvjll18APHhCa+3atYiKioKXlxfWrFmDFi1aAACcnJywbds2TJgwAc888wycnJzwyiuvYMmSJfK5hg4dipycHCxduhSTJk2Cu7s7+vfvr9wFEpGiVJIkSaIbQURUESqVCl999RVeeukl0U0hoiqCY4CIiIio2mEAIiIiomqHY4CIqMrjnXwiKiv2ABEREVG1wwBERERE1Q4DEBEREVU7DEBERERU7TAAERERUbXDAERERETVDgMQERERVTsMQERERFTtMAARERFRtfP/nRV3boUs9pEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot epoch vs loss\n",
    "plt.plot(range(1, num_epochs+1), losses, label='Training Loss vs Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss vs Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.50%\n",
      "True Positives: 339\n",
      "False Positives: 87\n",
      "True Negatives: 313\n",
      "False Negatives: 61\n",
      "Precision: 0.7958\n",
      "Specificty: 0.7825\n",
      "Recall: 0.8475\n",
      "F1 Score: 0.8208\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test set\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    val_outputs = model(X_test)\n",
    "    val_outputs_cls = (val_outputs >= 0.5).float()\n",
    "    val_accuracy = (val_outputs_cls == y_test).float().mean()\n",
    "    print(f'Test Accuracy: {val_accuracy.item()*100:.2f}%')\n",
    "    # Convert predicted outputs to binary predictions (0 or 1)\n",
    "    val_predictions = (val_outputs >= 0.5).float()\n",
    "\n",
    "    # True positives, false positives, true negatives, false negatives\n",
    "    TP = ((val_predictions == 1) & (y_test == 1)).sum().item()\n",
    "    FP = ((val_predictions == 1) & (y_test == 0)).sum().item()\n",
    "    TN = ((val_predictions == 0) & (y_test == 0)).sum().item()\n",
    "    FN = ((val_predictions == 0) & (y_test == 1)).sum().item()\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = TP / (TP + FP + 1e-10)  # Add small epsilon to avoid division by zero\n",
    "    recall = TP / (TP + FN + 1e-10)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)  # Add small epsilon to avoid division by zero\n",
    "    specificity = TN / (TN + FP)\n",
    "\n",
    "    # Print the evaluation metrics\n",
    "    print(f'True Positives: {TP}')\n",
    "    print(f'False Positives: {FP}')\n",
    "    print(f'True Negatives: {TN}')\n",
    "    print(f'False Negatives: {FN}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Specificty: {specificity:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "\n",
    "It seems that my neural network did pretty well! I implemented a wide variety of techniques learned in this class in the gradient descent algorithm for the backpropogation process, such as the bold driver, L2 regularization, stochastic GD(Adam), as well as minibatch gradient descent. I found that adding these techniques led to a higher accuracy score as well as a very fast convergence.\n",
    "\n",
    "I predicted 81.5% of my samples correctly, with no clear bias towards positive or negative predictions. I predicted 79.6% of actual positive predictions correctly, 78.3% of actual negative predictions correctly, and 84.75% of overall positives correctly. These are pretty satisfactory results given the conditions.\n",
    "\n",
    "\n",
    " Something that could be helpful in the future to improve my results is to implement cross validation as well to fine-tune my hyperparameters, as well as using a larger dataset. A very large-scale version of this project could include loops across cross-validations where I perform a linear regression connecting my hyperparameters to my accuracy score! However, this is extremely extremely expensive and time consuming, so I'm happy with just having an operational neural network for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.74875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.76      0.75       400\n",
      "         1.0       0.76      0.73      0.75       400\n",
      "\n",
      "    accuracy                           0.75       800\n",
      "   macro avg       0.75      0.75      0.75       800\n",
      "weighted avg       0.75      0.75      0.75       800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "LRmodel = LogisticRegression( fit_intercept=True)\n",
    "LRmodel.fit(X_train, y_train)\n",
    "LRpred = LRmodel.predict(X_test)\n",
    "print('Accuracy score: ' + str(accuracy_score(y_test, LRpred)))\n",
    "print(classification_report(y_test, LRpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that logistic regression works like a neural network, just as a very rudimentary one with one layer that is the sigmoid activation function. We can see that adding the extra layers as we did to form a bigger neural network improved prediction metrics all across the board when compared to just using Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.80875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.75      0.80       400\n",
      "         1.0       0.78      0.87      0.82       400\n",
      "\n",
      "    accuracy                           0.81       800\n",
      "   macro avg       0.81      0.81      0.81       800\n",
      "weighted avg       0.81      0.81      0.81       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "SVMmodel = svm.SVC()\n",
    "SVMmodel.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "SVMpred = SVMmodel.predict(X_test)\n",
    "\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, SVMpred))\n",
    "print(classification_report(y_test, SVMpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM comes close, but still a bit worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/sklearn/neighbors/_classification.py:215: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=22)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "KNNpred = knn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, KNNpred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN did not perform as well, but slightly better than logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train, y_train)\n",
    "TREEpred = tree_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_tree = accuracy_score(y_test, TREEpred)\n",
    "print(\"Accuracy:\", accuracy_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees did not perform as well. Pretty much the same as logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier Accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the AdaBoost Classifier\n",
    "adaboost_clf = AdaBoostClassifier()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "Y_pred_adaboost = adaboost_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_adaboost = accuracy_score(y_test, Y_pred_adaboost)\n",
    "print(\"AdaBoost Classifier Accuracy:\", accuracy_adaboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Very close! Even with an ensemble method like AdaBoost, it still is slightly below the accuracy I reached with my neural network. Furthermore, I would argue that there is no increase in interpretability in AdaBoost when compared to neural networks. Thus, even with the avengers of classic classifiers, it still couldn't surpass my neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
